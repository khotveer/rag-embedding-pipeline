# rag-embedding-pipeline
This repository showcases how to fine-tune the pretrained GPT-2 Medium model using LoRA (Low-Rank Adaptation) on a custom dataset consisting of Shakespeare plays. The goal is to demonstrate parameter-efficient fine-tuning with Hugging Faceâ€™s PEFT library and generate Shakespearean-style text using a compact, fast, and powerful model.
